MODEL_API_URL=http://localhost:11434/api/generate
# To use Ollama, ensure it's running: ollama serve
# Then pull a model: ollama pull llama2 (or similar)
# Default model name can be set below or passed per request
OLLAMA_MODEL=llama3.2:latest 
